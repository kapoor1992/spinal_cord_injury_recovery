{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\clark xu\\anaconda3\\lib\\site-packages (3.3.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\clark xu\\anaconda3\\lib\\site-packages (from lightgbm) (0.33.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\clark xu\\anaconda3\\lib\\site-packages (from lightgbm) (1.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\clark xu\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\clark xu\\anaconda3\\lib\\site-packages (from lightgbm) (0.21.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\clark xu\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dataset loaded from ../data/csvs/f1_public.csv\n",
      "output dataset loaded to ../data/csvs/translated_dataset.csv\n",
      "Dataset shape: (21049, 80)\n",
      "0.8990450852772103 : 0.10095491472278968 split for testing starting at injury date 2006.\n",
      "Accuracy on train set: 0.8618685267385331\n",
      "Accuracy on test set: 0.7152941176470589\n",
      "Precision on train set: 0.8430562592825199\n",
      "Precision on test set: 0.466065283706925\n",
      "Recall on train set: 0.6169163202153569\n",
      "Recall on test set: 0.4537585336732389\n",
      "F1-Score on train set: 0.6236285610508108\n",
      "F1-Score on test set: 0.4466021589607382\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import pickle\n",
    "from data import dataset_loader\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
    "\n",
    "def run_metrics(clf,train_x,train_y,test_x,test_y):\n",
    "    \"\"\"\n",
    "    Print ML classifier metrics on train and test\n",
    "    \"\"\"\n",
    "    accuracyTrain = clf.score(train_x, train_y)\n",
    "    print(f'Accuracy on train set: {accuracyTrain}')\n",
    "    \n",
    "    accuracy = clf.score(test_x, test_y)\n",
    "    print(f'Accuracy on test set: {accuracy}')\n",
    "\n",
    "    precisionTrain = precision_score(train_y, clf.predict(train_x), average=\"macro\")\n",
    "    print(f'Precision on train set: {precisionTrain}')\n",
    "    \n",
    "    precision = precision_score(test_y, clf.predict(test_x), average=\"macro\")\n",
    "    print(f'Precision on test set: {precision}')\n",
    "\n",
    "    recallTrain = recall_score(train_y, clf.predict(train_x), average=\"macro\")\n",
    "    print(f'Recall on train set: {recallTrain}')\n",
    "    \n",
    "    recall = recall_score(test_y, clf.predict(test_x), average=\"macro\")\n",
    "    print(f'Recall on test set: {recall}')\n",
    "    \n",
    "    f1Train = f1_score(train_y, clf.predict(train_x), average=\"macro\")\n",
    "    print(f'F1-Score on train set: {f1Train}')\n",
    "    \n",
    "    f1 = f1_score(test_y, clf.predict(test_x), average=\"macro\")\n",
    "    print(f'F1-Score on test set: {f1}')    \n",
    "    \n",
    "    #rocTrain = roc_auc_score(train_y, clf.predict(train_x))\n",
    "    #print(f'ROC on train set: {rocTrain}')\n",
    "    \n",
    "    #roc = roc_auc_score(test_y, clf.predict(test_x))\n",
    "    #print(f'ROC on test set: {rocTest}')\n",
    "\n",
    "def run_naive_bayes(train_x, train_y, test_x, test_y):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"Naive Bayes\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "    \n",
    "def run_logistic_regression(train_x, train_y, test_x, test_y):\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"Logistic Regression\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "    \n",
    "def run_elastic_net(train_x, train_y, test_x, test_y):\n",
    "    clf = LogisticRegression(penalty='elasticnet',max_iter=200,solver='saga',l1_ratio=.9)\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"Elastic Net\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "    \n",
    "def run_knn(train_x, train_y, test_x, test_y):\n",
    "    clf = KNeighborsClassifier(n_neighbors=9)\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"KNN\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "\n",
    "def run_decision_tree(train_x, train_y, test_x, test_y):\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"Decision Tree\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "    \n",
    "def run_random_forest(train_x, train_y, test_x, test_y, num_trees):\n",
    "    clf = RandomForestClassifier(n_estimators=num_trees)\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"Random Forest\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "    \n",
    "def run_xgboost(train_x, train_y, test_x, test_y, num_trees):\n",
    "    clf = GradientBoostingClassifier(n_estimators=num_trees)\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"XG Boost\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "    \n",
    "def run_adaboost(train_x, train_y, test_x, test_y, num_trees):\n",
    "    clf = AdaBoostClassifier(n_estimators=num_trees)\n",
    "    clf.fit(train_x, train_y)\n",
    "    print(\"ADA Boost\")\n",
    "    run_metrics(clf,train_x,train_y,test_x,test_y)\n",
    "    \n",
    "def run_ensemble(train_x, train_y, test_x, test_y):\n",
    "    #Elastic Net\n",
    "    el = LogisticRegression(penalty='elasticnet',max_iter=200,solver='saga',l1_ratio=.9)\n",
    "    el.fit(train_x, train_y)\n",
    "    \n",
    "    #Random Forest\n",
    "    rf = RandomForestClassifier(n_estimators=1000)\n",
    "    rf.fit(train_x, train_y)\n",
    "    \n",
    "    #KNN\n",
    "    knn = KNeighborsClassifier(n_neighbors=9)\n",
    "    knn.fit(train_x, train_y)    \n",
    "    \n",
    "    #Record Classifiers\n",
    "    estimators=[('elastic-net',el), ('random-forest',rf), ('k-nearest-neighbors',knn)]\n",
    "\n",
    "    #Create Ensemble\n",
    "    ensemble = VotingClassifier(estimators,voting='hard')\n",
    "    ensemble.fit(train_x, train_y)\n",
    "    run_metrics(ensemble,train_x,train_y,test_x,test_y)\n",
    "\n",
    "def main():\n",
    "    # Only do these lines once.\n",
    "    np.random.seed(2)\n",
    "    df = dataset_loader.get_dataset_df('../data/csvs/f1_public.csv', '../data/csvs/translated_dataset.csv')\n",
    "    train_x, train_y, test_x, test_y = dataset_loader.get_train_test_split(df)\n",
    "    \n",
    "    # Add model runs here.\n",
    "    #run_naive_bayes(train_x, train_y, test_x, test_y)\n",
    "    #run_logistic_regression(train_x, train_y, test_x, test_y)\n",
    "    #run_elastic_net(train_x, train_y, test_x, test_y)\n",
    "    #run_knn(train_x, train_y, test_x, test_y)\n",
    "    #run_decision_tree(train_x, train_y, test_x, test_y)\n",
    "    #run_random_forest(train_x, train_y, test_x, test_y, num_trees=1000)\n",
    "    #run_xgboost(train_x, train_y, test_x, test_y, num_trees=1000)\n",
    "    #run_adaboost(train_x, train_y, test_x, test_y, num_trees=1000)\n",
    "    run_ensemble(train_x, train_y, test_x, test_y)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
